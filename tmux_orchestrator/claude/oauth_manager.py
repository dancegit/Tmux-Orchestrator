"""
OAuth Port Management Module and Implementation Specifications

CRITICAL MODULE: Contains OAuth port timing logic that prevents port 3000 conflicts 
during batch processing. These timeouts were carefully calibrated through extensive 
testing and MUST NOT be reduced.

This module handles:
- OAuth port availability detection
- Port conflict resolution during batch processing
- Enhanced timing controls for Claude OAuth server shutdown
- Diagnostic information for debugging port conflicts

MODULAR ADDITION: Implementation specification data models
- Specification parsing and validation
- Project configuration structures
- Implementation plan management
"""

import os
import subprocess
import time
import re
import sqlite3
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from pydantic import BaseModel, Field
from rich.console import Console
from rich.panel import Panel
from rich.table import Table

console = Console()


# ============================================================================
# IMPLEMENTATION SPECIFICATION DATA MODELS
# 
# Modular implementation of specification parsing and validation functionality
# previously embedded in the legacy monolithic system system.
# 
# These Pydantic models provide structured parsing of JSON implementation specs
# generated by Claude during project analysis.
# ============================================================================

class Phase(BaseModel):
    """Individual phase within an implementation plan."""
    name: str
    duration_hours: float
    tasks: List[str]

class ImplementationPlan(BaseModel):
    """Complete implementation plan with phases and time estimates."""
    phases: List[Phase]
    total_estimated_hours: float

class RoleConfig(BaseModel):
    """Configuration for an individual agent role."""
    responsibilities: List[str]
    check_in_interval: int
    initial_commands: List[str]

class Project(BaseModel):
    """Project metadata and configuration."""
    name: str
    path: str
    type: str
    project_type: Optional[str] = None  # Alias for type, used by briefing system
    tech_stack: Optional[List[str]] = None  # Alias for main_tech, used by briefing system
    main_tech: List[str]
    description: Optional[str] = None  # Project description for briefings

class GitWorkflow(BaseModel):
    """Git workflow configuration for the project."""
    parent_branch: str = "main"  # The branch we started from
    branch_name: str
    commit_interval: int
    pr_title: str

class ProjectSize(BaseModel):
    """Project size estimation and complexity assessment."""
    size: str = Field(default="medium", description="small|medium|large")
    estimated_loc: int = Field(default=1000)
    complexity: str = Field(default="medium")

class ImplementationSpec(BaseModel):
    """
    Complete implementation specification parsed from Claude analysis.
    
    This is the primary data structure that orchestrates project setup,
    team deployment, and workflow configuration. It contains all the
    structured information needed to convert a natural language specification
    into a concrete implementation plan with assigned roles.
    """
    project: Project
    implementation_plan: ImplementationPlan
    roles: Dict[str, RoleConfig]
    git_workflow: GitWorkflow
    success_criteria: List[str]
    project_size: ProjectSize = Field(default_factory=ProjectSize)


# ============================================================================
# IMPLEMENTATION PLAN DISPLAY FUNCTIONALITY
# 
# Modular implementation of plan display and approval functionality
# previously embedded in the legacy monolithic system system.
# 
# These functions provide comprehensive plan visualization and user approval
# workflows for orchestration setup.
# ============================================================================

class PlanDisplayManager:
    """
    Manages the display of implementation plans with rich formatting.
    
    This class provides comprehensive plan visualization including project
    overview, implementation phases, role assignments, and approval workflows.
    It replaces the legacy AutoOrchestrator.display_implementation_plan method.
    """
    
    def __init__(self, project_path: Optional[str] = None):
        """
        Initialize the plan display manager.
        
        Args:
            project_path: Optional project path for context
        """
        self.project_path = project_path
        self.console = Console()
    
    def display_implementation_plan(self, spec: ImplementationSpec, 
                                   manual_size: Optional[str] = None,
                                   additional_roles: Optional[List[str]] = None,
                                   plan_type: str = 'max5') -> bool:
        """
        Display the implementation plan and get user approval.
        
        This method provides comprehensive visualization of the implementation
        plan including project overview, phases, roles, success criteria,
        and git workflow. It auto-approves by default for automated setup.
        
        Args:
            spec: Parsed implementation specification
            manual_size: Optional manual size override
            additional_roles: Optional additional roles requested
            plan_type: Subscription plan type for token usage calculations
            
        Returns:
            bool: True (always auto-approves for automated workflow)
        """
        # Project overview
        self.console.print(Panel.fit(
            f"[bold cyan]{spec.project.name}[/bold cyan]\n"
            f"Path: {spec.project.path}\n"
            f"Type: {spec.project.type}\n"
            f"Technologies: {', '.join(spec.project.main_tech)}",
            title="📋 Project Overview"
        ))
        
        # Implementation phases
        phases_table = Table(title="Implementation Phases")
        phases_table.add_column("Phase", style="cyan")
        phases_table.add_column("Duration", style="green")
        phases_table.add_column("Tasks", style="yellow")
        
        for phase in spec.implementation_plan.phases:
            tasks_str = "\n".join(f"• {task}" for task in phase.tasks)
            phases_table.add_row(
                phase.name,
                f"{phase.duration_hours}h",
                tasks_str
            )
        
        self.console.print(phases_table)
        self.console.print(f"\n[bold]Total Estimated Time:[/bold] {spec.implementation_plan.total_estimated_hours} hours\n")
        
        # Project size info
        size_info = f"Project Size: [yellow]{spec.project_size.size}[/yellow]"
        if manual_size and manual_size != spec.project_size.size:
            size_info += f" (overridden from auto-detected: {spec.project_size.size})"
        self.console.print(size_info)
        
        if additional_roles:
            self.console.print(f"Additional Roles Requested: [cyan]{', '.join(additional_roles)}[/cyan]")
        
        self.console.print()
        
        # Roles and responsibilities
        roles_table = Table(title="Role Assignments")
        roles_table.add_column("Role", style="cyan")
        roles_table.add_column("Check-in", style="green")
        roles_table.add_column("Responsibilities", style="yellow")
        
        for role_name, role_config in spec.roles.items():
            resp_str = "\n".join(f"• {resp}" for resp in role_config.responsibilities[:3])
            if len(role_config.responsibilities) > 3:
                resp_str += f"\n• ... and {len(role_config.responsibilities) - 3} more"
            roles_table.add_row(
                role_name.title(),
                f"{role_config.check_in_interval}m",
                resp_str
            )
        
        self.console.print(roles_table)
        
        # Success criteria
        self.console.print(Panel(
            "\n".join(f"✓ {criterion}" for criterion in spec.success_criteria),
            title="🎯 Success Criteria",
            border_style="green"
        ))
        
        # Git workflow
        git_panel_content = [
            f"Parent Branch: [yellow]{spec.git_workflow.parent_branch}[/yellow] (current branch)",
            f"Feature Branch: [cyan]{spec.git_workflow.branch_name}[/cyan]",
            f"Commit Interval: Every {spec.git_workflow.commit_interval} minutes",
            f"PR Title: {spec.git_workflow.pr_title}"
        ]
        
        if spec.git_workflow.parent_branch != "main":
            git_panel_content.append(f"[bold red]⚠️  Will merge back to {spec.git_workflow.parent_branch}, NOT main![/bold red]")
        
        self.console.print(Panel(
            "\n".join(git_panel_content),
            title="🔀 Git Workflow"
        ))
        
        # Show which roles will actually be deployed
        roles_to_deploy = self.get_roles_for_project_size(spec, plan_type, additional_roles)
        self.console.print(f"\n[bold]Roles to be deployed:[/bold] {', '.join([r[0] for r in roles_to_deploy])}")
        
        # Show if development was detected
        if self._analyze_tasks_for_dev(spec):
            self.console.print("[green]✓ Development tasks detected - Developer and Tester roles enforced[/green]")
        
        # Token usage warning
        team_size = len(roles_to_deploy)
        if team_size >= 4:
            self.console.print(f"\n[yellow]⚠️  Token Usage Warning[/yellow]")
            self.console.print(f"[yellow]Running {team_size} agents concurrently will use ~{team_size * 15}x normal token consumption[/yellow]")
            self.console.print(f"[yellow]On {plan_type} plan, this provides approximately {225 // (team_size * 15)} messages per 5-hour session[/yellow]")
            
            if team_size >= 5 and plan_type == 'max5':
                self.console.print(f"[red]Consider using fewer agents or upgrading to max20 plan for extended sessions[/red]")
        
        # Auto-approve setup - no manual confirmation needed
        self.console.print("\n[green]✓ Proceeding with automated setup...[/green]")
        return True
    
    def _analyze_tasks_for_dev(self, spec: ImplementationSpec) -> bool:
        """
        Analyzes all tasks in the implementation plan to detect if development (coding) is needed.
        
        Uses keyword/regex matching for indicators like file creation, implementation, or testing.
        
        Args:
            spec: Implementation specification to analyze
            
        Returns:
            bool: True if development tasks detected
        """
        dev_indicators = [
            r'implement', r'create.*\.py', r'write.*(test|code)', r'integrate', r'develop', r'build',
            r'coding', r'function', r'class', r'module', r'api', r'endpoint'
        ]
        pattern = re.compile('|'.join(dev_indicators), re.IGNORECASE)
        
        # Check all tasks in all phases
        for phase in spec.implementation_plan.phases:
            for task in phase.tasks:
                if pattern.search(task):
                    return True
        
        # Also check project type
        if spec.project.type in ['python', 'javascript', 'go', 'java']:
            return True
            
        return False
    
    def get_plan_constraints(self, plan_type: str) -> int:
        """
        Get maximum recommended team size based on subscription plan.
        
        Args:
            plan_type: Subscription plan type
            
        Returns:
            Maximum number of concurrent agents for sustainable token usage
        """
        plan_limits = {
            'pro': 3,      # Pro plan: Max 3 agents (limited tokens)
            'max5': 5,     # Max 5x plan: Max 5 agents (balanced)
            'max20': 8,    # Max 20x plan: Max 8 agents (more headroom)
            'console': 10  # Console/Enterprise: Higher limits
        }
        
        return plan_limits.get(plan_type, 5)
    
    def get_roles_for_project_size(self, spec: ImplementationSpec, 
                                  plan_type: str = 'max5',
                                  additional_roles: Optional[List[str]] = None) -> List[Tuple[str, str]]:
        """
        Determine which roles to deploy based on project size and plan constraints.
        
        This is a simplified version that returns the core team for most projects.
        For full dynamic team composition, the system would need to integrate
        with the DynamicTeamComposer module.
        
        Args:
            spec: Implementation specification
            plan_type: Subscription plan type
            additional_roles: Optional additional roles requested
            
        Returns:
            List of tuples (display_name, role_key) to deploy
        """
        # Role mapping for display names
        role_mapping = {
            'orchestrator': ('Orchestrator', 'orchestrator'),
            'project_manager': ('Project-Manager', 'project_manager'),
            'developer': ('Developer', 'developer'),
            'tester': ('Tester', 'tester'),
            'testrunner': ('TestRunner', 'testrunner'),
            'researcher': ('Researcher', 'researcher'),
            'documentation_writer': ('Documentation', 'documentation_writer'),
            'devops': ('DevOps', 'devops'),
            'code_reviewer': ('Code-Reviewer', 'code_reviewer'),
            'logtracker': ('LogTracker', 'logtracker'),
            # System operations roles
            'sysadmin': ('SysAdmin', 'sysadmin'),
            'securityops': ('SecurityOps', 'securityops'),
            'networkops': ('NetworkOps', 'networkops'),
            'monitoringops': ('MonitoringOps', 'monitoringops'),
            'databaseops': ('DatabaseOps', 'databaseops')
        }
        
        # If custom roles are specified, use them
        if additional_roles:
            self.console.print(f"\n[bold]Using custom role selection[/bold]")
            selected_roles = []
            
            # Always include orchestrator if not explicitly added
            if 'orchestrator' not in [r.lower() for r in additional_roles]:
                selected_roles.append(role_mapping['orchestrator'])
            
            # Add requested roles
            for role in additional_roles:
                role_lower = role.lower()
                if role_lower in role_mapping:
                    if role_mapping[role_lower] not in selected_roles:
                        selected_roles.append(role_mapping[role_lower])
                else:
                    self.console.print(f"[yellow]Warning: Unknown role '{role}' - skipping[/yellow]")
        else:
            # Default simplified team (core roles)
            self.console.print(f"\n[bold]Using default team composition[/bold]")
            selected_roles = [
                role_mapping['orchestrator'],
                role_mapping['project_manager'],
                role_mapping['developer'],
                role_mapping['tester'],
                role_mapping['testrunner']
            ]
        
        # Detect if development is needed and enforce core roles
        needs_dev = self._analyze_tasks_for_dev(spec)
        if needs_dev:
            has_developer = any(role[1] == 'developer' for role in selected_roles)
            has_tester = any(role[1] == 'tester' for role in selected_roles)
            
            if not has_developer:
                selected_roles.append(role_mapping['developer'])
                self.console.print(f"[yellow]⚠️  Detected coding tasks - Adding Developer role (required)[/yellow]")
            
            if not has_tester:
                selected_roles.append(role_mapping['tester'])
                self.console.print(f"[yellow]⚠️  Detected coding tasks - Adding Tester role (required)[/yellow]")
        
        # Enforce plan constraints
        max_agents = self.get_plan_constraints(plan_type)
        if len(selected_roles) > max_agents:
            self.console.print(f"\n[yellow]⚠️  Warning: {len(selected_roles)} agents recommended but {plan_type} plan supports max {max_agents}[/yellow]")
            self.console.print(f"[yellow]Team will be limited to {max_agents} agents to prevent token exhaustion[/yellow]")
            self.console.print(f"[yellow]Multi-agent systems use ~15x more tokens than standard usage[/yellow]\n")
            
            # Prioritize core roles
            priority_order = ['orchestrator', 'project_manager', 'developer', 'tester', 'testrunner']
            
            # Sort roles by priority
            prioritized_roles = []
            for priority_role in priority_order:
                for role in selected_roles:
                    if role[1] == priority_role and role not in prioritized_roles:
                        prioritized_roles.append(role)
                        if len(prioritized_roles) >= max_agents:
                            break
                if len(prioritized_roles) >= max_agents:
                    break
            
            # Add any remaining roles up to the limit
            for role in selected_roles:
                if role not in prioritized_roles:
                    prioritized_roles.append(role)
                    if len(prioritized_roles) >= max_agents:
                        break
            
            selected_roles = prioritized_roles
        
        return selected_roles


# ============================================================================
# BATCH QUEUE MANAGEMENT FUNCTIONALITY
# 
# Modular implementation of batch queue management functionality
# previously delegated to legacy the modular orchestrator from line 149.
# 
# This eliminates OAuth port conflicts by providing proper scheduler
# initialization and project queuing for batch mode processing.
# ============================================================================

class BatchQueueManager:
    """
    Manages batch queue operations for orchestration projects.
    
    This class provides modular batch queue management, replacing the legacy
    delegation to the modular orchestrator from line 149 of orchestrator.py.
    It handles project enqueueing, scheduler initialization, and proper
    database management to prevent OAuth port conflicts during batch processing.
    """
    
    def __init__(self, tmux_orchestrator_path: Path):
        """
        Initialize the batch queue manager.
        
        Args:
            tmux_orchestrator_path: Path to Tmux Orchestrator installation
        """
        self.tmux_orchestrator_path = Path(tmux_orchestrator_path)
        self.console = Console()
        self.scheduler = None
        self.queue_manager = None
        self.db_connection = None
        self._init_modular_components()
    
    def _init_modular_components(self):
        """
        Initialize the modular CoreScheduler with proper database setup.
        
        This uses the modular scheduler implementation instead of the legacy
        TmuxOrchestratorScheduler, eliminating delegation to the modular orchestrator
        and providing better separation of concerns.
        """
        try:
            # Ensure scheduler modules are importable
            import sys
            if str(self.tmux_orchestrator_path) not in sys.path:
                sys.path.insert(0, str(self.tmux_orchestrator_path))
            
            # Import modular scheduler components
            from scheduler_modules.core_scheduler import CoreScheduler
            from scheduler_modules.config import SchedulerConfig
            from scheduler_modules.queue_manager import QueueManager
            
            # Use the class directly - it has class attributes for configuration
            config = SchedulerConfig
            
            # Initialize database connection
            db_path = self.tmux_orchestrator_path / 'task_queue.db'
            self.db_connection = sqlite3.connect(str(db_path), check_same_thread=False)
            self.db_connection.execute("PRAGMA journal_mode=WAL")  # Better concurrency
            
            # Ensure database schema exists
            self._ensure_database_schema()
            
            # Initialize the modular queue manager
            self.queue_manager = QueueManager(
                db_connection=self.db_connection,
                config=config,
                lock_manager=None  # Optional lock manager
            )
            
            # Initialize modular scheduler for compatibility
            self.scheduler = CoreScheduler(
                db_path=str(db_path),
                tmux_orchestrator_path=self.tmux_orchestrator_path,
                config=config
            )
            
            self.console.print(f"[green]✓ Modular batch queue components initialized with database: {db_path}[/green]")
            self.console.print(f"[green]✓ Using QueueManager and CoreScheduler instead of legacy TmuxOrchestratorScheduler[/green]")
            
        except ImportError as e:
            self.console.print(f"[red]❌ Failed to import modular components: {e}[/red]")
            raise RuntimeError(f"Cannot initialize modular batch queue manager: {e}")
        except Exception as e:
            self.console.print(f"[red]❌ Failed to initialize modular components: {e}[/red]")
            raise RuntimeError(f"Modular component initialization failed: {e}")
    
    def _ensure_database_schema(self):
        """
        Ensure the database schema exists for the project queue.
        
        This creates the necessary tables if they don't exist, providing
        compatibility with the modular QueueManager.
        """
        cursor = self.db_connection.cursor()
        
        # Create project_queue table if it doesn't exist
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS project_queue (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                spec_path TEXT NOT NULL,
                project_path TEXT,
                status TEXT DEFAULT 'queued',
                priority INTEGER DEFAULT 5,
                enqueued_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                started_at TIMESTAMP,
                completed_at TIMESTAMP,
                enqueued_at REAL,
                error_message TEXT,
                session_name TEXT,
                batch_id TEXT,
                orchestrator_session TEXT,
                main_session TEXT
            )
        """)
        
        # Create indexes for better performance
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_project_queue_status 
            ON project_queue(status)
        """)
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_project_queue_priority 
            ON project_queue(priority DESC, enqueued_at ASC)
        """)
        
        self.db_connection.commit()
        self.console.print("[green]✓ Database schema verified/created[/green]")
    
    def enqueue_project(self, spec_file: Path, project_path: Optional[Path] = None,
                       batch_id: Optional[str] = None, priority: int = 5) -> Optional[int]:
        """
        Add a project to the batch queue for processing.
        
        This is the modular implementation of the functionality from orchestrator.py
        lines 158-166, providing proper project enqueueing without delegation.
        Uses the modular QueueManager instead of legacy scheduler.
        
        Args:
            spec_file: Path to the specification file
            project_path: Optional path to the project directory
            batch_id: Optional batch identifier for grouping projects
            priority: Queue priority (1-10, higher = more urgent)
            
        Returns:
            int: Project ID if successfully enqueued, None otherwise
        """
        if not self.queue_manager:
            self.console.print("[red]❌ Queue manager not initialized - cannot enqueue project[/red]")
            return None
        
        try:
            # Validate spec file exists
            if not spec_file.exists():
                self.console.print(f"[red]❌ Spec file not found: {spec_file}[/red]")
                return None
            
            # Prepare metadata including batch_id
            metadata = {}
            if batch_id:
                metadata['batch_id'] = batch_id
            
            # Enqueue the project using modular QueueManager
            project_id = self.queue_manager.enqueue_project(
                spec_path=str(spec_file),
                project_path=str(project_path) if project_path else "",
                priority=priority,
                metadata=metadata
            )
            
            if project_id:
                self.console.print(f"[green]✓ Project enqueued with ID: {project_id}[/green]")
                self.console.print(f"  Spec: {spec_file}")
                if project_path:
                    self.console.print(f"  Path: {project_path}")
                if batch_id:
                    self.console.print(f"  Batch: {batch_id}")
                self.console.print(f"  Priority: {priority}")
            else:
                self.console.print("[yellow]⚠️  Project enqueueing returned no ID[/yellow]")
            
            return project_id
            
        except Exception as e:
            self.console.print(f"[red]❌ Failed to enqueue project: {e}[/red]")
            import traceback
            traceback.print_exc()
            return None
    
    def get_queue_status(self) -> Dict[str, Any]:
        """
        Get the current status of the batch queue.
        
        Returns:
            Dict containing queue statistics and pending projects
        """
        if not self.db_connection:
            return {'error': 'Database connection not initialized'}
        
        try:
            # Get queue statistics using the existing database connection
            cursor = self.db_connection.cursor()
            
            # Count projects by status
            cursor.execute("""
                SELECT status, COUNT(*) as count 
                FROM project_queue 
                GROUP BY status
            """)
            status_counts = dict(cursor.fetchall())
            
            # Get pending projects
            cursor.execute("""
                SELECT id, spec_path, priority, enqueued_at 
                FROM project_queue 
                WHERE status = 'queued' 
                ORDER BY priority DESC, enqueued_at ASC 
                LIMIT 10
            """)
            pending = cursor.fetchall()
            
            return {
                'status_counts': status_counts,
                'pending_projects': [
                    {
                        'id': p[0],
                        'spec_path': p[1],
                        'priority': p[2],
                        'enqueued_at': p[3]
                    } for p in pending
                ],
                'total_pending': status_counts.get('queued', 0)
            }
            
        except Exception as e:
            return {'error': str(e)}
    
    def process_next_project(self) -> bool:
        """
        Process the next project in the batch queue.
        
        This provides manual queue progression for testing and recovery.
        
        Returns:
            bool: True if a project was processed, False if queue is empty
        """
        if not self.queue_manager:
            self.console.print("[red]❌ Queue manager not initialized[/red]")
            return False
        
        try:
            # Get next project from queue using modular queue manager
            cursor = self.db_connection.cursor()
            cursor.execute("""
                SELECT id, spec_path, project_path, priority 
                FROM project_queue 
                WHERE status = 'queued' 
                ORDER BY priority DESC, enqueued_at ASC 
                LIMIT 1
            """)
            next_project = cursor.fetchone()
            
            if not next_project:
                self.console.print("[yellow]No projects in queue[/yellow]")
                return False
            
            project_id, spec_path, project_path, priority = next_project
            self.console.print(f"[cyan]Processing project {project_id}: {spec_path}[/cyan]")
            
            # Update status to processing
            cursor.execute("""
                UPDATE project_queue 
                SET status = 'processing', started_at = datetime('now') 
                WHERE id = ?
            """, (project_id,))
            self.db_connection.commit()
            
            # Trigger orchestration (simplified - actual implementation would launch tmux session)
            self.console.print(f"[green]✓ Project {project_id} marked for processing[/green]")
            self.console.print("[yellow]Note: Actual orchestration launch would happen here[/yellow]")
            
            return True
            
        except Exception as e:
            self.console.print(f"[red]❌ Failed to process next project: {e}[/red]")
            return False
    
    def clear_failed_projects(self) -> int:
        """
        Clear failed projects from the queue to allow retry.
        
        Returns:
            int: Number of failed projects cleared
        """
        if not self.db_connection:
            return 0
        
        try:
            cursor = self.db_connection.cursor()
            
            # Count failed projects
            cursor.execute("SELECT COUNT(*) FROM project_queue WHERE status = 'failed'")
            count = cursor.fetchone()[0]
            
            # Clear failed projects
            cursor.execute("DELETE FROM project_queue WHERE status = 'failed'")
            self.db_connection.commit()
            
            if count > 0:
                self.console.print(f"[green]✓ Cleared {count} failed projects from queue[/green]")
            
            return count
            
        except Exception as e:
            self.console.print(f"[red]❌ Failed to clear failed projects: {e}[/red]")
            return 0

# ============================================================================
# TMUX SESSION ORCHESTRATION FUNCTIONALITY
# 
# Modular implementation of tmux session setup and orchestration functionality
# previously embedded in the legacy monolithic system system.
# 
# These classes provide session creation, agent deployment, and coordination
# workflows for orchestration setup.
# ============================================================================

class SessionOrchestrator:
    """
    Orchestrates tmux session setup with agents and worktrees.
    
    This class provides comprehensive session orchestration including tmux
    window creation, agent deployment, briefing, and coordination. It replaces
    the legacy AutoOrchestrator.setup_tmux_session method with a modular approach.
    """
    
    def __init__(self, tmux_orchestrator_path: Path, project_path: Path):
        """
        Initialize the session orchestrator.
        
        Args:
            tmux_orchestrator_path: Path to Tmux Orchestrator installation
            project_path: Path to the project being orchestrated
        """
        self.tmux_orchestrator_path = Path(tmux_orchestrator_path)
        self.project_path = Path(project_path)
        self.console = Console()
        
        # Initialize modular components
        from ..core.session_manager import SessionManager
        from ..tmux.session_controller import TmuxSessionController
        from ..agents.agent_factory import AgentFactory
        from ..agents.briefing_system import BriefingSystem, BriefingContext, ProjectSpec
        from ..agents.agent_factory import RoleConfig
        from ..git.worktree_manager import WorktreeManager
        from ..tmux.messaging import TmuxMessenger
        
        self.session_manager = SessionManager(self.tmux_orchestrator_path)
        self.tmux_controller = TmuxSessionController(self.tmux_orchestrator_path)
        self.agent_factory = AgentFactory(self.tmux_orchestrator_path)
        self.worktree_manager = WorktreeManager(self.tmux_orchestrator_path)
        self.messenger = TmuxMessenger(self.tmux_orchestrator_path)
        
        # OAuth manager for port management
        self.oauth_manager = OAuthManager()
        
        # Session tracking
        self.unique_session_name = None
        self.worktree_paths = {}
    
    def _find_existing_session(self, spec: ImplementationSpec) -> Optional[str]:
        """
        Check for existing tmux sessions for the same project to prevent duplicates.
        
        Args:
            spec: Implementation specification with project details
            
        Returns:
            str: Existing session name if found, None otherwise
        """
        import subprocess
        import re
        
        try:
            # Get all tmux sessions
            result = subprocess.run(['tmux', 'list-sessions', '-F', '#{session_name}'], 
                                 capture_output=True, text=True)
            
            if result.returncode != 0:
                return None
                
            existing_sessions = result.stdout.strip().split('\n')
            if not existing_sessions or existing_sessions == ['']:
                return None
            
            # Create pattern to match project-related sessions
            project_name = re.sub(r'[^a-zA-Z0-9-]', '-', spec.project.name)[:20]
            pattern = re.compile(f"^{re.escape(project_name)}-[a-f0-9]{{8}}$")
            
            # Check for existing sessions matching this project
            for session in existing_sessions:
                session = session.strip()
                if pattern.match(session):
                    # Verify the session is still active and has windows
                    check_result = subprocess.run(
                        ['tmux', 'list-windows', '-t', session, '-F', '#{window_name}'],
                        capture_output=True, text=True
                    )
                    if check_result.returncode == 0:
                        windows = check_result.stdout.strip().split('\n')
                        if windows and len(windows) > 1:  # Has multiple windows (likely agents)
                            self.console.print(f"[yellow]⚠️  Found existing session for this project: {session}[/yellow]")
                            return session
            
            return None
            
        except Exception as e:
            self.console.print(f"[yellow]Warning: Could not check for existing sessions: {e}[/yellow]")
            return None

    def setup_tmux_session(self, spec: ImplementationSpec, team_config: Optional[Dict[str, Any]] = None) -> str:
        """
        Set up the tmux session with roles based on project specification.
        
        This method provides a simplified modular version of the legacy setup_tmux_session,
        focusing on core functionality while leveraging modular components.
        
        Args:
            spec: Implementation specification with project details
            team_config: Optional team configuration overrides
            
        Returns:
            str: Session name if successful, None otherwise
        """
        try:
            # Check for existing session first
            existing_session = self._find_existing_session(spec)
            if existing_session:
                self.console.print(f"[blue]🔄 Found existing session for project: {existing_session}[/blue]")
                self.console.print(f"[blue]Reusing existing session instead of creating duplicate[/blue]")
                self.unique_session_name = existing_session
                return existing_session
            
            # Generate unique session name
            import uuid
            import re
            project_name = re.sub(r'[^a-zA-Z0-9-]', '-', spec.project.name)[:20]
            unique_id = str(uuid.uuid4())[:8]
            self.unique_session_name = f"{project_name}-{unique_id}"
            
            self.console.print(f"[blue]🚀 Setting up orchestration session: {self.unique_session_name}[/blue]")
            
            # Determine which roles to deploy
            plan_manager = PlanDisplayManager(str(self.project_path))
            roles_to_deploy = plan_manager.get_roles_for_project_size(
                spec, 
                plan_type=team_config.get('plan_type', 'max5') if team_config else 'max5',
                additional_roles=team_config.get('additional_roles') if team_config else None
            )
            
            # Set up git worktrees for isolation
            self.console.print("[cyan]Setting up git worktrees for agent isolation...[/cyan]")
            self.worktree_paths = self._setup_worktrees(spec, roles_to_deploy)
            
            # Validate critical resources in worktrees
            if not self._validate_worktree_resources(roles_to_deploy):
                self.console.print("[yellow]⚠️  Warning: Some critical resources are missing in worktrees[/yellow]")
                # Note: Continue execution but log the warning for now
                # In future, consider making this a hard failure
            
            # Create tmux session with windows
            self.console.print("[cyan]Creating tmux session and windows...[/cyan]")
            if not self._create_tmux_session(roles_to_deploy):
                raise RuntimeError("Failed to create tmux session")
            
            # Brief all agents (simplified version)
            self.console.print("[cyan]Briefing agents...[/cyan]")
            self._brief_agents(spec, roles_to_deploy)
            
            # Success message
            self.console.print(f"\n[green]✓ Tmux session '{self.unique_session_name}' created successfully![/green]")
            self.console.print(f"Project size: [yellow]{spec.project_size.size}[/yellow]")
            self.console.print(f"Deployed roles: {', '.join([r[0] for r in roles_to_deploy])}")
            self.console.print(f"\nTo attach: [cyan]tmux attach -t {self.unique_session_name}[/cyan]")
            self.console.print(f"\n[yellow]Note: Each agent works in their own git worktree[/yellow]")
            
            return self.unique_session_name
            
        except Exception as e:
            self.console.print(f"[red]❌ Failed to set up session: {e}[/red]")
            import traceback
            traceback.print_exc()
            return None
    
    def _setup_worktrees(self, spec: ImplementationSpec, roles_to_deploy: List[Tuple[str, str]]) -> Dict[str, Path]:
        """
        Set up git worktrees for agent isolation.
        
        Args:
            spec: Implementation specification
            roles_to_deploy: List of (display_name, role_key) tuples
            
        Returns:
            Dict mapping role_key to worktree path
        """
        worktree_paths = {}
        
        # Create worktree base directory
        worktree_base = self.project_path.parent / f"{self.project_path.name}-tmux-worktrees"
        worktree_base.mkdir(exist_ok=True)
        
        for display_name, role_key in roles_to_deploy:
            worktree_path = worktree_base / role_key
            
            if not worktree_path.exists():
                try:
                    # Create worktree (simplified - actual implementation would handle branches)
                    import subprocess
                    result = subprocess.run(
                        ['git', '-C', str(self.project_path), 'worktree', 'add', 
                         str(worktree_path), '-b', f'{role_key}-branch'],
                        capture_output=True, text=True
                    )
                    
                    if result.returncode != 0:
                        # Try without creating new branch
                        subprocess.run(
                            ['git', '-C', str(self.project_path), 'worktree', 'add', 
                             str(worktree_path)],
                            capture_output=True
                        )
                    
                    self.console.print(f"[green]✓ Created worktree for {role_key}[/green]")
                    
                    # Copy .mcp.json if it exists in the project
                    project_mcp = self.project_path / '.mcp.json'
                    if project_mcp.exists():
                        worktree_mcp = worktree_path / '.mcp.json'
                        try:
                            import shutil
                            shutil.copy2(project_mcp, worktree_mcp)
                            self.console.print(f"[green]✓ Copied .mcp.json to {role_key}'s worktree[/green]")
                        except Exception as e:
                            self.console.print(f"[yellow]Warning: Could not copy .mcp.json to {role_key}: {e}[/yellow]")
                    
                    # Also copy CLAUDE.md if it exists
                    project_claude_md = self.project_path / 'CLAUDE.md'
                    if project_claude_md.exists():
                        worktree_claude_md = worktree_path / 'CLAUDE.md'
                        try:
                            import shutil
                            shutil.copy2(project_claude_md, worktree_claude_md)
                            self.console.print(f"[green]✓ Copied CLAUDE.md to {role_key}'s worktree[/green]")
                        except Exception as e:
                            self.console.print(f"[yellow]Warning: Could not copy CLAUDE.md to {role_key}: {e}[/yellow]")
                    
                except Exception as e:
                    self.console.print(f"[yellow]⚠️  Using project directory for {role_key}: {e}[/yellow]")
                    worktree_path = self.project_path
            
            worktree_paths[role_key] = worktree_path
        
        return worktree_paths
    
    def _validate_worktree_resources(self, roles_to_deploy: List[Tuple[str, str]]) -> bool:
        """
        Validate that critical resources exist in worktrees.
        
        Args:
            roles_to_deploy: List of (display_name, role_key) tuples
            
        Returns:
            bool: True if all critical resources exist, False otherwise
        """
        all_valid = True
        missing_resources = []
        
        for display_name, role_key in roles_to_deploy:
            if role_key not in self.worktree_paths:
                continue
                
            worktree_path = self.worktree_paths[role_key]
            
            # Check for CLAUDE.md (optional but recommended)
            claude_md_path = worktree_path / 'CLAUDE.md'
            if not claude_md_path.exists():
                missing_resources.append(f"{role_key}/CLAUDE.md")
                self.console.print(f"[yellow]⚠️  Missing CLAUDE.md in {role_key}'s worktree[/yellow]")
                # Don't fail for missing CLAUDE.md, just warn
            
            # Add more critical resource checks here if needed
            # For example, check for .git directory, required config files, etc.
            
        if missing_resources:
            self.console.print(f"[yellow]Missing resources: {', '.join(missing_resources)}[/yellow]")
            # For now, we'll return True to continue execution
            # In future versions, consider making this return False for critical failures
            
        return all_valid
    
    def _create_tmux_session(self, roles_to_deploy: List[Tuple[str, str]]) -> bool:
        """
        Create tmux session with windows for each role.
        
        Args:
            roles_to_deploy: List of (display_name, role_key) tuples
            
        Returns:
            bool: True if successful
        """
        import subprocess
        
        # Check if session already exists
        check_result = subprocess.run(
            ['tmux', 'has-session', '-t', self.unique_session_name],
            capture_output=True
        )
        
        if check_result.returncode == 0:
            # Session exists, try to kill it first
            subprocess.run(['tmux', 'kill-session', '-t', self.unique_session_name], capture_output=True)
            time.sleep(1)
        
        # Create first window with orchestrator
        first_window, first_role = roles_to_deploy[0]
        working_dir = str(self.worktree_paths.get(first_role, self.project_path))
        
        result = subprocess.run([
            'tmux', 'new-session', '-d', '-s', self.unique_session_name,
            '-n', first_window, '-c', working_dir
        ], capture_output=True, text=True)
        
        if result.returncode != 0:
            self.console.print(f"[red]Failed to create session: {result.stderr}[/red]")
            return False
        
        # Create other windows
        for idx, (window_name, role_key) in enumerate(roles_to_deploy[1:], start=1):
            working_dir = str(self.worktree_paths.get(role_key, self.project_path))
            subprocess.run([
                'tmux', 'new-window', '-t', self.unique_session_name,
                '-n', window_name, '-c', working_dir
            ], capture_output=True)
        
        return True
    
    def _is_port_free(self, port: int) -> bool:
        """Check if a port is available for use."""
        import socket
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            try:
                s.bind(('', port))
                return True
            except OSError:
                return False
    
    def _brief_agents(self, spec: ImplementationSpec, roles_to_deploy: List[Tuple[str, str]]):
        """
        Send initial briefings to all agents using the BriefingSystem.
        
        Args:
            spec: Implementation specification
            roles_to_deploy: List of (display_name, role_key) tuples
        """
        import subprocess
        import os
        import time
        from pathlib import Path
        # Import BriefingSystem locally to ensure it's in scope
        from ..agents.briefing_system import BriefingSystem, BriefingContext, ProjectSpec as BriefingProjectSpec
        
        # Initialize BriefingSystem
        briefing_system = BriefingSystem(self.tmux_orchestrator_path)
        
        # Create ProjectSpec from ImplementationSpec
        project_spec = BriefingProjectSpec(
            name=spec.project.name,
            path=str(spec.project.path),
            type=spec.project.project_type or "generic",
            main_tech=spec.project.tech_stack or [],
            description=spec.project.description or ""
        )
        
        for idx, (window_name, role_key) in enumerate(roles_to_deploy):
            window_target = f"{self.unique_session_name}:{idx}"
            worktree_path = self.worktree_paths.get(role_key, self.project_path)
            
            # Check if MCP configuration exists for this worktree
            mcp_json_path = Path(worktree_path) / '.mcp.json'
            needs_mcp_approval = mcp_json_path.exists()
            
            if needs_mcp_approval:
                self.console.print(f"[cyan]Pre-initializing Claude for {window_name} to approve MCP servers...[/cyan]")
                
                # Check OAuth port availability first
                oauth_port = int(os.environ.get('CLAUDE_OAUTH_PORT', '3000'))
                if not self._is_port_free(oauth_port):
                    self.console.print(f"[yellow]Warning: OAuth port {oauth_port} in use, waiting...[/yellow]")
                    time.sleep(5)
                
                # Step 1: Start Claude normally (without --dangerously-skip-permissions) for MCP approval
                subprocess.run([
                    'tmux', 'send-keys', '-t', window_target,
                    'claude', 'Enter'
                ], capture_output=True, text=True)
                
                # Wait for MCP server prompt to appear
                time.sleep(2)
                
                # Step 2: Press 'y' to accept MCP servers
                subprocess.run([
                    'tmux', 'send-keys', '-t', window_target,
                    'y'
                ], capture_output=True, text=True)
                
                time.sleep(0.5)
                
                # Step 3: Press Enter to confirm
                subprocess.run([
                    'tmux', 'send-keys', '-t', window_target,
                    'Enter'
                ], capture_output=True, text=True)
                
                # Wait for Claude to fully start
                time.sleep(2)
                
                # Step 4: Press Escape to ensure we're not in any input mode
                subprocess.run([
                    'tmux', 'send-keys', '-t', window_target,
                    'Escape'
                ], capture_output=True, text=True)
                
                time.sleep(0.5)
                
                # Step 5: Exit Claude
                subprocess.run([
                    'tmux', 'send-keys', '-t', window_target,
                    '/exit', 'Enter'
                ], capture_output=True, text=True)
                
                # Wait for Claude to exit and OAuth port to be released
                time.sleep(3)
                
                # Step 6: Kill and recreate the window
                subprocess.run([
                    'tmux', 'kill-window', '-t', window_target
                ], capture_output=True, text=True)
                
                # Wait for port to be fully released
                time.sleep(2)
                
                # Recreate the window
                subprocess.run([
                    'tmux', 'new-window', '-t', window_target,
                    '-n', window_name, '-c', str(worktree_path)
                ], capture_output=True, text=True)
                
                # Small delay before starting Claude again
                time.sleep(1)
            
            # Start Claude with --dangerously-skip-permissions
            result = subprocess.run([
                'tmux', 'send-keys', '-t', window_target,
                'claude --dangerously-skip-permissions', 'Enter'
            ], capture_output=True, text=True)
            
            if result.returncode != 0:
                self.console.print(f"[yellow]Warning: Failed to start Claude in {window_name}: {result.stderr}[/yellow]")
            
            # Wait for Claude to start
            time.sleep(5)
            
            # Generate comprehensive briefing using BriefingSystem
            if role_key in spec.roles:
                role_config_data = spec.roles[role_key]
                
                # Create RoleConfig from spec data
                role_config = RoleConfig(
                    role_key=role_key,
                    window_name=window_name,
                    agent_prompt=role_config_data.agent_prompt if hasattr(role_config_data, 'agent_prompt') else "",
                    responsibilities=role_config_data.responsibilities if hasattr(role_config_data, 'responsibilities') else [],
                    constraints=role_config_data.constraints if hasattr(role_config_data, 'constraints') else [],
                    autonomy_level=role_config_data.autonomy_level if hasattr(role_config_data, 'autonomy_level') else "guided",
                    enable_code_execution=role_config_data.enable_code_execution if hasattr(role_config_data, 'enable_code_execution') else True,
                    worktree_branch=role_config_data.worktree_branch if hasattr(role_config_data, 'worktree_branch') else None
                )
                
                # Build team members list
                team_members = [(name, i) for i, (name, _) in enumerate(roles_to_deploy)]
                
                # Create BriefingContext
                context = BriefingContext(
                    project_spec=project_spec,
                    role_config=role_config,
                    session_name=self.unique_session_name,
                    worktree_path=Path(worktree_path),
                    team_members=team_members,
                    git_branch=role_config.worktree_branch or "main",
                    enable_mcp=needs_mcp_approval
                )
                
                # Generate comprehensive briefing
                briefing = briefing_system.generate_role_briefing(context)
                
                # Send the comprehensive briefing
                self.console.print(f"[cyan]Sending comprehensive briefing to {window_name}...[/cyan]")
                
                # Use the send-direct-message.sh script if available
                send_script = Path("/home/clauderun/.claude/scripts/send-direct-message.sh")
                if send_script.exists():
                    # Save briefing to temp file
                    import tempfile
                    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt') as f:
                        f.write(briefing)
                        temp_path = f.name
                    
                    # Send using the script
                    result = subprocess.run([
                        str(send_script),
                        self.unique_session_name,
                        str(idx),
                        f"< {temp_path}"
                    ], capture_output=True, text=True, shell=True)
                    
                    # Clean up temp file
                    Path(temp_path).unlink()
                    
                    if result.returncode != 0:
                        self.console.print(f"[yellow]Warning: Failed to send briefing via script: {result.stderr}[/yellow]")
                        # Fallback to line-by-line sending
                        self._send_briefing_fallback(window_target, briefing)
                else:
                    # Fallback to line-by-line sending
                    self._send_briefing_fallback(window_target, briefing)
                
                self.console.print(f"[green]✓ Sent comprehensive briefing to {window_name}[/green]")
            else:
                self.console.print(f"[yellow]Warning: No role config found for {role_key}[/yellow]")
    
    def _send_briefing_fallback(self, window_target: str, briefing: str):
        """Fallback method to send briefing line by line."""
        import subprocess
        import time
        
        for line in briefing.split('\n'):
            if line:
                # Prevent hyphen interpretation by adding space prefix if line starts with hyphen
                if line.startswith('-'):
                    safe_line = ' ' + line
                else:
                    safe_line = line
                
                result = subprocess.run([
                    'tmux', 'send-keys', '-t', window_target,
                    safe_line
                ], capture_output=True, text=True)
                
                if result.returncode != 0:
                    self.console.print(f"[yellow]Warning: Failed to send line: {result.stderr}[/yellow]")
                
                # Send Enter key separately
                subprocess.run([
                    'tmux', 'send-keys', '-t', window_target, 'Enter'
                ], capture_output=True, text=True)
                time.sleep(0.05)

class OAuthManager:
    """
    Manages OAuth port operations with enhanced batch processing support.
    
    CRITICAL: This class contains timing sequences that prevent port 3000 conflicts
    during batch processing. All timeouts and waits have been calibrated through
    extensive testing and should not be modified without comprehensive testing.
    """
    
    def __init__(self, oauth_port: int = None, port_pool: List[int] = None):
        """
        Initialize OAuth manager with port pooling support.
        
        Args:
            oauth_port: Primary OAuth port to manage (defaults to CLAUDE_OAUTH_PORT env var or 3000)
            port_pool: Pool of ports for parallel agent initialization (defaults to [3000, 3001, 3002, 3003, 3004])
        """
        self.oauth_port = oauth_port or int(os.environ.get('CLAUDE_OAUTH_PORT', '3000'))
        self.port_pool = port_pool or [3000, 3001, 3002, 3003, 3004]
        console.print(f"[blue]🔧 OAuth Manager initialized with port pool: {self.port_pool}[/blue]")
    
    def is_port_free(self, port: int = None) -> bool:
        """
        Check if a port is available using multiple detection methods.
        
        Uses multiple approaches for maximum reliability:
        1. netstat command (most reliable)
        2. ss command (modern alternative) 
        3. lsof command (fallback)
        
        Args:
            port: Port to check (defaults to self.oauth_port)
            
        Returns:
            bool: True if port is free, False if in use
        """
        port = port or self.oauth_port
        
        # Method 1: netstat (most widely available)
        try:
            result = subprocess.run(
                ['netstat', '-tuln'], 
                capture_output=True, text=True, timeout=5
            )
            if result.returncode == 0:
                return f":{port} " not in result.stdout and f":{port}\t" not in result.stdout
        except Exception:
            pass
        
        # Method 2: ss (modern netstat replacement)
        try:
            result = subprocess.run(
                ['ss', '-tuln'], 
                capture_output=True, text=True, timeout=5
            )
            if result.returncode == 0:
                return f":{port} " not in result.stdout and f":{port}\t" not in result.stdout
        except Exception:
            pass
        
        # Method 3: lsof (process-focused)
        try:
            result = subprocess.run(
                ['lsof', '-i', f'TCP:{port}'], 
                capture_output=True, text=True, timeout=5
            )
            if result.returncode == 0:
                return f":{port}" not in result.stdout
        except Exception:
            # If all commands fail, assume port is free (optimistic)
            return True

    def wait_for_port_free(self, port: int = None, max_wait: int = 120) -> bool:
        """
        Wait for OAuth port to become free with enhanced batch processing support.
        
        CRITICAL: This function prevents port 3000 conflicts during batch processing.
        DO NOT reduce polling frequency or timeouts - they are calibrated for:
        
        1. Claude OAuth server shutdown timing (5-15 seconds)
        2. System socket release under load (10-30 seconds) 
        3. Batch processing race conditions (additional 15-30 seconds)
        4. Network stack delays in containerized environments
        
        The 0.5-second polling interval balances responsiveness with system load.
        Longer intervals cause unnecessary delays; shorter intervals waste CPU.
        
        Args:
            port: Port to wait for (defaults to self.oauth_port)
            max_wait: Maximum wait time in seconds
            
        Returns:
            bool: True if port became free within timeout, False otherwise
        """
        port = port or self.oauth_port
        
        console.print(f"[yellow]⏳ Waiting for port {port} to become free (max {max_wait}s)...[/yellow]")
        
        start_time = time.time()
        consecutive_free_checks = 0
        required_consecutive = 3  # Require 3 consecutive "free" checks to avoid race conditions
        
        while (time.time() - start_time) < max_wait:
            if self.is_port_free(port):
                consecutive_free_checks += 1
                if consecutive_free_checks >= required_consecutive:
                    elapsed = time.time() - start_time
                    console.print(f"[green]✓ Port {port} is now free (after {elapsed:.1f}s)[/green]")
                    return True
            else:
                consecutive_free_checks = 0
            
            # Enhanced polling with exponential backoff for efficiency
            if consecutive_free_checks > 0:
                time.sleep(0.2)  # Faster polling when port is becoming free
            else:
                time.sleep(0.5)  # Standard polling interval
        
        elapsed = time.time() - start_time
        console.print(f"[red]❌ Port {port} still in use after {elapsed:.1f}s[/red]")
        return False

    def check_batch_processing_conflict(self, session_name: str) -> bool:
        """
        Detect and report OAuth port conflicts during batch processing.
        
        This function provides early detection of port conflicts before starting
        MCP initialization. Early detection prevents:
        - Wasted time on failed Claude starts (60+ seconds per failure)
        - Cascading delays in batch processing queues
        - Silent failures that require manual project resets
        
        Args:
            session_name: Name of the session being processed
            
        Returns:
            bool: True if port is available, False if conflict detected
        """
        if not self.is_port_free(self.oauth_port):
            console.print(f"[red]❌ BATCH PROCESSING CONFLICT DETECTED![/red]")
            console.print(f"[red]OAuth port {self.oauth_port} is already in use - cannot start MCP initialization[/red]")
            
            # Enhanced diagnostic for batch processing conflicts
            self._diagnose_port_conflict()
            
            console.print(f"[red]Aborting MCP initialization for {session_name}[/red]")
            console.print(f"[red]This will prevent session briefing - project will likely need reset[/red]")
            return False
        
        console.print(f"[green]✓ OAuth port {self.oauth_port} is available - proceeding with MCP initialization[/green]")
        return True

    def wait_after_window_kill(self, max_wait: int = 120) -> bool:
        """
        Wait for OAuth port release after killing a tmux window.
        
        CRITICAL OAUTH PORT WAIT - DO NOT REDUCE THESE TIMEOUTS!
        
        After killing the tmux window, Claude's OAuth server on port 3000 takes
        time to fully release. During BATCH PROCESSING, multiple projects compete
        for the same port, causing "port already in use" conflicts.
        
        Root causes requiring this wait:
        1. Claude OAuth server graceful shutdown (5-10 seconds)
        2. System socket cleanup and TIME_WAIT state (10-20 seconds)  
        3. Batch processing (multiple projects) amplifies race conditions
        4. Without this wait, the next Claude start fails silently
        
        MINIMUM 30 SECONDS REQUIRED - verified through batch testing
        
        Args:
            max_wait: Maximum wait time in seconds (default 45s)
            
        Returns:
            bool: True if port became free, False if timeout
        """
        console.print(f"[yellow]⏳ Waiting for OAuth port {self.oauth_port} to be released after window kill...[/yellow]")
        
        success = self.wait_for_port_free(self.oauth_port, max_wait=max_wait)
        
        if not success:
            console.print(f"[red]⚠️ WARNING: OAuth port {self.oauth_port} still in use after {max_wait}s![/red]")
            console.print(f"[red]This may cause the next Claude start to fail.[/red]")
            
            # Enhanced diagnostic information
            self._diagnose_port_conflict()
            
            # Continue anyway but log the issue
            console.print(f"[yellow]Continuing with potential port conflict...[/yellow]")
            return False
        
        return True

    def wait_after_claude_exit(self, max_wait: int = 120) -> bool:
        """
        Wait for OAuth port release after Claude exits with exponential backoff.
        
        CRITICAL OAUTH PORT WAIT AFTER CLAUDE EXIT - Enhanced with exponential backoff!
        
        After sending /exit to Claude, the OAuth server needs time to shut down
        and release port 3000. This is ESPECIALLY critical during BATCH PROCESSING
        where multiple projects are starting Claude instances in rapid succession.
        
        Technical details requiring the 120-second timeout:
        1. Claude graceful shutdown process (10-20s)
        2. OAuth server cleanup and socket release (10-20s)
        3. Node.js event loop cleanup (5-10s)
        4. System socket release under load (10-30s)
        5. Buffer for batch processing delays (additional 15-20s)
        6. ENHANCED: Exponential backoff for efficiency (NEW)
        
        MINIMUM 120 SECONDS REQUIRED for reliable batch processing
        
        Args:
            max_wait: Maximum wait time in seconds (default 120s)
            
        Returns:
            bool: True if port became free, False if timeout
        """
        console.print(f"[yellow]⏳ Waiting for Claude OAuth server on port {self.oauth_port} to shut down...[/yellow]")
        
        # Enhanced exponential backoff implementation
        backoff = 1  # Start with 1s delay
        total_wait = 0
        port = self.oauth_port
        
        while total_wait < max_wait:
            # Check if port is free using multiple detection methods
            if self.is_port_free(port):
                console.print(f"[green]✓ OAuth port {port} available after {total_wait}s[/green]")
                return True
            
            console.print(f"[yellow]⏳ Waiting for OAuth port {port} release... ({total_wait}s/{max_wait}s)[/yellow]")
            
            # Exponential backoff with cap
            time.sleep(backoff)
            total_wait += backoff
            backoff = min(backoff * 2, 10)  # Double delay, cap at 10s
        
        # Timeout reached - enhanced diagnostics
        console.print(f"[red]⚠️ CRITICAL: OAuth port {self.oauth_port} still in use after {max_wait}s![/red]")
        console.print(f"[red]This WILL cause batch processing failures and port conflicts![/red]")
        console.print("[red]Suggestion: Check for stuck Claude processes with 'ps aux | grep claude' and kill if needed[/red]")
        
        # Enhanced debugging for persistent port blocks
        self._diagnose_persistent_port_block()
        return False

    def _diagnose_port_conflict(self):
        """Provide diagnostic information for OAuth port conflicts."""
        try:
            result = subprocess.run(['lsof', '-i', f'TCP:{self.oauth_port}'], 
                                  capture_output=True, text=True, timeout=2)
            if result.stdout.strip():
                lines = result.stdout.strip().split('\n')
                if len(lines) > 1:  # Skip header
                    process_info = lines[1].split()
                    if len(process_info) >= 2:
                        cmd = process_info[0]
                        pid = process_info[1]
                        console.print(f"[red]Port blocked by: {cmd} (PID {pid})[/red]")
                        
                        # If it's another Claude process, this is likely a batch conflict
                        if 'claude' in cmd.lower():
                            console.print(f"[yellow]⚠️  This appears to be a BATCH PROCESSING CONFLICT[/yellow]")
                            console.print(f"[yellow]Another Claude instance is using port {self.oauth_port}[/yellow]")
                            console.print(f"[yellow]Recommendation: Wait for other project to complete MCP init[/yellow]")
        except Exception:
            pass

    def _diagnose_persistent_port_block(self):
        """Provide enhanced diagnostics for persistent port blocks."""
        try:
            result = subprocess.run(['lsof', '-i', f'TCP:{self.oauth_port}'], 
                                  capture_output=True, text=True, timeout=2)
            if result.stdout.strip():
                console.print(f"[red]Port {self.oauth_port} blocked by: {result.stdout.strip()}[/red]")
                
                # Try to identify the PID for potential cleanup
                lines = result.stdout.strip().split('\n')
                for line in lines[1:]:  # Skip header
                    if 'claude' in line.lower():
                        parts = line.split()
                        if len(parts) >= 2:
                            pid = parts[1]
                            console.print(f"[yellow]Found Claude process PID {pid} blocking port[/yellow]")
                            console.print(f"[yellow]Consider: kill -TERM {pid} (if safe to do so)[/yellow]")
                            break
        except Exception:
            pass
    
    def pre_claude_start_check(self, role_key: str = "agent") -> bool:
        """
        Proactive OAuth port conflict check before starting Claude.
        
        This prevents the entire MCP initialization sequence from running only
        to fail later during the window kill/recreate step. During batch processing,
        this early detection saves 60+ seconds per failed project and prevents
        cascading delays.
        
        Args:
            role_key: Role identifier for logging purposes
            
        Returns:
            bool: True if port is available and Claude can start, False otherwise
        """
        console.print(f"[cyan]Pre-checking OAuth port for {role_key}...[/cyan]")
        
        if not self.is_port_free():
            console.print(f"[red]❌ BATCH PROCESSING CONFLICT DETECTED![/red]")
            console.print(f"[red]OAuth port {self.oauth_port} is already in use - cannot start MCP initialization[/red]")
            
            # Enhanced diagnostic for batch processing conflicts
            try:
                result = subprocess.run(['lsof', '-i', f'TCP:{self.oauth_port}'], 
                                      capture_output=True, text=True, timeout=2)
                if result.stdout.strip():
                    lines = result.stdout.strip().split('\n')
                    if len(lines) > 1:  # Skip header
                        process_info = lines[1].split()
                        if len(process_info) >= 2:
                            cmd, pid = process_info[0], process_info[1]
                            console.print(f"[red]Port blocked by: {cmd} (PID {pid})[/red]")
                            
                            # If it's another Claude process, this is likely a batch conflict
                            if 'claude' in cmd.lower():
                                console.print(f"[yellow]⚠️  This appears to be a BATCH PROCESSING CONFLICT[/yellow]")
                                console.print(f"[yellow]Another Claude instance is using port {self.oauth_port}[/yellow]")
                                console.print(f"[yellow]Recommendation: Wait for other project to complete MCP init[/yellow]")
            except Exception:
                pass
                
            console.print(f"[red]Aborting MCP initialization for {role_key}[/red]")
            console.print(f"[red]This will prevent session briefing - project will likely need reset[/red]")
            return False
        
        console.print(f"[green]✓ OAuth port {self.oauth_port} is available - proceeding with MCP initialization[/green]")
        return True
    
    def wait_after_claude_shutdown(self, max_wait: int = 120) -> bool:
        """
        Critical OAuth port wait after Claude /exit command.
        
        After sending /exit to Claude, the OAuth server needs time to shut down
        and release port 3000. This is ESPECIALLY critical during BATCH PROCESSING
        where multiple projects are starting Claude instances in rapid succession.
        
        This wait prevents the "port already in use" errors that cause:
        - Session creation to complete but agents never get briefed
        - Projects marked as COMPLETED but actually failed during setup  
        - Batch failures that require manual reset/re-queuing
        
        The 60-second timeout accounts for:
        - Claude's graceful shutdown process (5-10s)
        - OAuth server cleanup (5-15s)
        - System socket release under load (10-30s) 
        - Buffer for batch processing delays (additional 15-20s)
        
        MINIMUM 60 SECONDS REQUIRED for reliable batch processing.
        
        Args:
            max_wait: Maximum wait time in seconds (minimum 60 recommended)
            
        Returns:
            bool: True if port became free within timeout
        """
        console.print(f"[yellow]⏳ Waiting for Claude OAuth server on port {self.oauth_port} to shut down...[/yellow]")
        
        if not self.wait_for_port_free(max_wait=max_wait):
            console.print(f"[red]⚠️ CRITICAL: OAuth port {self.oauth_port} still in use after {max_wait}s![/red]")
            console.print(f"[red]This WILL cause batch processing failures and port conflicts![/red]")
            
            # Enhanced debugging for persistent port blocks
            try:
                result = subprocess.run(['lsof', '-i', f'TCP:{self.oauth_port}'], 
                                      capture_output=True, text=True, timeout=2)
                if result.stdout.strip():
                    console.print(f"[red]Port {self.oauth_port} blocked by: {result.stdout.strip()}[/red]")
                    
                    # Try to identify the PID for potential cleanup
                    lines = result.stdout.strip().split('\n')
                    for line in lines[1:]:  # Skip header
                        if 'claude' in line.lower():
                            parts = line.split()
                            if len(parts) > 1:
                                pid = parts[1]
                                console.print(f"[red]Claude process PID {pid} may need manual cleanup[/red]")
            except Exception:
                pass
            console.print(f"[yellow]Continuing but this may cause the next MCP initialization to fail...[/yellow]")
            return False
        else:
            console.print(f"[green]✓ OAuth server cleanly shut down[/green]")
            return True
    
    def get_port_status(self) -> Dict[str, Any]:
        """
        Get comprehensive OAuth port status information.
        
        Returns:
            Dict containing:
            - port: The OAuth port number
            - is_free: Whether the port is currently available
            - conflicts: List of processes using the port
            - recommendations: List of suggested actions
        """
        is_free = self.is_port_free()
        conflicts = []
        recommendations = []
        
        if not is_free:
            # Find processes using the port
            try:
                result = subprocess.run(
                    ['lsof', '-i', f'TCP:{self.oauth_port}'],
                    capture_output=True, text=True, timeout=5
                )
                if result.stdout:
                    conflicts = result.stdout.strip().split('\n')[1:]  # Skip header
            except:
                pass
            
            # Generate recommendations
            recommendations = [
                f"Kill processes using port {self.oauth_port}",
                f"Or set CLAUDE_OAUTH_PORT to a different port",
                f"Or wait for the current OAuth process to complete (45-60 seconds)"
            ]
            
            # Add process-specific recommendations
            for conflict in conflicts:
                if 'claude' in conflict.lower():
                    parts = conflict.split()
                    if len(parts) > 1:
                        pid = parts[1]
                        recommendations.append(f"Consider: kill -TERM {pid} (Claude process)")
        
        return {
            'port': self.oauth_port,
            'is_free': is_free,
            'conflicts': conflicts,
            'recommendations': recommendations
        }
    
    def wait_for_port_available(self, max_wait: int = 120) -> bool:
        """
        Enhanced port availability check with comprehensive diagnostics.
        
        This method combines wait_for_port_free with enhanced error reporting
        and automatic conflict resolution suggestions.
        
        Args:
            max_wait: Maximum wait time in seconds
            
        Returns:
            bool: True if port became available
        """
        if self.is_port_free():
            return True
        
        console.print(f"[yellow]OAuth port {self.oauth_port} is in use - waiting for release...[/yellow]")
        
        # Show what's using the port
        status = self.get_port_status()
        if status['conflicts']:
            console.print("[yellow]Current port usage:[/yellow]")
            for conflict in status['conflicts']:
                console.print(f"  {conflict}")
        
        # Wait for port to become free
        success = self.wait_for_port_free(max_wait=max_wait)
        
        if not success:
            console.print(f"[red]Port {self.oauth_port} did not become available within {max_wait}s[/red]")
            console.print("[yellow]Recommendations:[/yellow]")
            for rec in status['recommendations']:
                console.print(f"  - {rec}")
        
        return success
    
    def acquire_port_from_pool(self) -> Optional[int]:
        """
        Acquire a free port from the pool for parallel agent initialization.
        
        This enables multiple agents to start simultaneously without OAuth conflicts.
        Essential for future parallel deployment capabilities.
        
        Returns:
            int: Available port from pool, None if all ports busy
        """
        console.print("[cyan]🔍 Acquiring port from pool for agent initialization...[/cyan]")
        
        for port in self.port_pool:
            if self.is_port_free(port):
                console.print(f"[green]✓ Acquired port {port} from pool[/green]")
                return port
        
        console.print("[red]❌ No ports available in pool - reverting to sequential mode[/red]")
        return None
    
    def release_port_to_pool(self, port: int, max_wait: int = 120) -> bool:
        """
        Release a port back to the pool after agent initialization.
        
        Args:
            port: Port to release
            max_wait: Maximum wait time for release
            
        Returns:
            bool: True if port was successfully released
        """
        console.print(f"[cyan]🔓 Releasing port {port} back to pool...[/cyan]")
        return self.wait_for_port_free(port, max_wait)
    
    def get_available_ports(self) -> List[int]:
        """
        Get list of currently available ports from the pool.
        
        Returns:
            List of available port numbers
        """
        return [port for port in self.port_pool if self.is_port_free(port)]